<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Training and emergent compilation | VSAVM</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Fraunces:wght@400;600&family=Space+Grotesk:wght@400;500;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../assets/site.css">
  </head>
  <body>
    <div class="site">
      <header class="header">
        <div class="brand">VSAVM</div>
        <nav class="nav">
          <a href="../index.html">Home</a>
          <a href="../specs/">Specs</a>
          <a href="../theory/index.html">Theory</a>
          <a href="../wiki/index.html">Wiki</a>
        </nav>
      </header>
      <main class="main content">
        <h1>Training and emergent compilation</h1>
<p>This page is a theory note. It expands the topic in short chapters and defines terminology without duplicating the formal specification documents.</p><p>The diagram has a transparent background and is intended to be read together with the caption and the sections below.</p>
<p>Related wiki pages: <a href="../wiki/vm.html">VM</a>, <a href="../wiki/event-stream.html">event stream</a>, <a href="../wiki/context-scope.html">context scope</a>, <a href="../wiki/mdl.html">MDL</a>, <a href="../wiki/rl.html">RL</a>, <a href="../wiki/llm.html">LLM</a>, <a href="../wiki/macro-token.html">macro-unit</a>.</p>
<p>Related specs: <a href="../specs-viewer.html?doc=specs/DS005-training-learning-optimization.md">DS005</a>, <a href="../specs-viewer.html?doc=specs/DS010-emergent-separator-discovery.md">DS010</a>, <a href="../specs-viewer.html?doc=specs/DS011-generative-decoding.md">DS011</a>, <a href="../specs-viewer.html?doc=specs/DS012-disk-backed-storage.md">DS012</a>.</p>

<h2>Overview</h2>
<p>VSAVM treats “compilation” as a learned capability driven by compression pressure. Repeated patterns create incentives to represent intent as reusable executable programs (inner loop, DS005) and as reusable surface continuations (outer loop, DS011). Crucially, scope boundaries must emerge from structure (DS010 / NFS11), so learning does not rely on hardcoded topical partitions.</p>

<h2>The two loops (what exists today)</h2>
<ul>
  <li><strong>Inner loop (DS005)</strong>: pattern mining, schema proposal, consolidation, and rule learning. The repository contains the inner-loop building blocks (PatternMiner / SchemaProposer / Consolidator) and a coordinator (<code>TrainingService</code>).</li>
  <li><strong>Outer loop (DS011)</strong>: a macro-unit language model trained to continue byte sequences under budgets, optionally conditioned on VM state. The current concrete implementation is <code>MacroUnitModel</code> (<code>src/training/outer-loop/macro-unit-model.mjs</code>).</li>
</ul>
<p>The loops are compatible by design: inner-loop consolidation can produce more reusable units and programs; outer-loop continuation benefits from stable reversible macro-units. Not every integration point is wired into every harness yet, so the practical pipeline is documented below.</p>

<h2>Practical training pipeline (eval_tinyLLM)</h2>
<p><code>eval_tinyLLM</code> is the “ground truth” harness for today’s reproducible training and comparisons. It trains a small TensorFlow byte-level Transformer and VSAVM’s macro-unit model on the same prepared dataset and writes timestamped reports.</p>

<h3>Step-by-step: from raw text to a comparison report</h3>
<ol>
  <li><strong>Fetch data</strong>: download a raw dataset into <code>eval_tinyLLM/cache/</code>.</li>
  <li><strong>Prepare a split</strong>: create <code>train.txt</code> and <code>valid.txt</code> under a deterministic <code>datasetId</code> (keyed by maxBytes/trainRatio/textField).</li>
  <li><strong>Train VSAVM macro-units</strong>: stream bytes from <code>train.txt</code> and train <code>MacroUnitModel.trainStream</code>. Optionally ingest facts into the VM, but large runs typically use <code>--skip-ingest</code> to focus on the language-model comparison.</li>
  <li><strong>Train TF baseline</strong>: train a minimal byte-level Transformer (kept small on purpose so training stays feasible).</li>
  <li><strong>Evaluate</strong>: compute perplexity and auxiliary metrics for both engines.</li>
  <li><strong>Compare</strong>: run a budgeted prompt suite and write an HTML+JSON report to <code>eval_tinyLLM/results/&lt;timestamp&gt;_results.html</code>.</li>
</ol>

<h3>Artifacts are versioned by dataset size and model configuration</h3>
<p>Prepared datasets and trained models are stored under <code>eval_tinyLLM/cache/</code> so multiple sizes and multiple model variants can coexist without overwriting:</p>
<ul>
  <li><strong>Datasets</strong>: <code>cache/datasets/&lt;datasetId&gt;/train.txt</code>, <code>valid.txt</code>, <code>meta.json</code> + a <code>latest.json</code> pointer.</li>
  <li><strong>Models</strong>: <code>cache/models/{vsavm,tf}/&lt;datasetId&gt;/&lt;modelId&gt;/</code> with <code>meta.json</code> and per-engine artifacts + <code>latest.json</code> pointers.</li>
  <li><strong>Results</strong>: timestamped <code>results/&lt;timestamp&gt;_results.html</code> and <code>.json</code> reports for comparisons.</li>
</ul>
<p>This is what makes size-based comparisons realistic: you can train multiple variants on different byte budgets and compare them without manually moving files.</p>

<h2>Scaling guidance (larger datasets, RAM constraints)</h2>
<p>The outer-loop macro-unit model is designed to stream training data, but it still maintains in-memory n-gram maps and subsequence counters. Large datasets are therefore feasible only with explicit caps and pruning.</p>
<ul>
  <li><strong>Use streaming training</strong>: <code>MacroUnitModel.trainStream</code> processes one record at a time (no full corpus in RAM).</li>
  <li><strong>Cap n-gram order</strong>: keep <code>maxNgramOrder</code> small (default in the harness is 8) to prevent combinatorial growth.</li>
  <li><strong>Prune aggressively</strong>: increase <code>minFrequency</code> / <code>pruneThreshold</code> on larger runs.</li>
  <li><strong>Sample subsequences</strong>: set <code>subsequenceSampleRate</code> to trade accuracy for memory/time.</li>
  <li><strong>Guard the heap</strong>: use <code>eval_tinyLLM/tools/run-with-ram.mjs</code> and/or <code>train-large.mjs</code> to pick a safe <code>--max-old-space-size</code> automatically.</li>
</ul>
<p>Disk-backed fact storage (DS012) reduces RAM pressure when ingesting and persisting <em>facts</em>. It does not currently move language-model n-gram state to disk.</p>

<h2>Risks and mitigations</h2>
<p>Compression can consolidate spurious patterns if prediction quality is the only criterion. VSAVM mitigates this by (a) scoping via DS010 so unstable patterns do not contaminate unrelated regions and (b) correctness checks (DS004) when translating learned structure into executable commitments.</p>
<figure class="diagram">
<img class="diagram-svg" src="../assets/svg/training-and-emergence-diagram.svg" alt="training-and-emergence diagram">
<figcaption>Compilation emerges when prediction pressure makes compact representations the cheapest explanation for recurring patterns. Inner-loop consolidation targets executable programs; outer-loop consolidation targets reversible macro-units for continuation.</figcaption>
</figure>
<h2>References</h2>
<p><a href="https://en.wikipedia.org/wiki/Minimum_description_length">Minimum description length (Wikipedia)</a> <a href="https://www.grunwald.nl/mdlbook/">The MDL Book (Grunwald)</a> <a href="https://en.wikipedia.org/wiki/Program_synthesis">Program synthesis (Wikipedia)</a> <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">Reinforcement learning (Wikipedia)</a></p>
      </main>
      <footer class="footer">
        VSAVM is an Axiologic Research experiment within the Achilles project. This static documentation is written in clear academic English for engineers.
      </footer>
    </div>
  </body>
</html>
