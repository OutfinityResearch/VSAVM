# DS011 Generative Decoding and Macro-Unit Language Modeling

This document specifies the generative decoding module that enables VSAVM to produce LLM-like text continuations while remaining faithful to VM state and correctness contracts. It integrates with DS005's two-loop training architecture and DS010's emergent separator discovery.

DS011 is additive: it does not change core semantics. It operationalizes the "outer loop" from DS005 and connects macro-unit continuation to the inner loop's pattern mining and schema consolidation.

## Goals

- Provide a concrete, minimal decoding pipeline that satisfies FS06/FS07.
- Integrate with DS005's two-loop architecture (outer: prediction, inner: program search).
- Use MDL-based macro-unit discovery for compression-driven phrase learning.
- Preserve modality-agnostic scope discovery (DS010/NFS11).
- Keep strict-mode determinism (NFS02).
- Prevent unsupported claims in generated output (DS004/DS001).
- Avoid new external dependencies (NFS).

## Non-Goals

- No domain-specific or modality-specific decoding logic.
- No hardcoded scope paths or separator types.
- No direct inference of "truth" from statistical decoding alone.
- No replacement of the inner loop's pattern mining.

## Implementation Status (This Repository)

This DS describes the target generative decoding architecture. The repository currently implements a pragmatic subset to keep the core dependency-light and to support reproducible comparisons.

Implemented:

- `MacroUnitModel` (`src/training/outer-loop/macro-unit-model.mjs`): byte-level macro-unit discovery + continuation, streaming training (`trainStream`), bounded n-gram orders, pruning/sampling, and reversible encode/decode.
- `VMStateConditioner` (`src/generation/vm-state-conditioner.mjs`): deterministic structural conditioning signature.
- `ClaimGate` (`src/generation/constraints/claim-gate.mjs`): optional proposal validation against VM state claims/facts.
- `eval_tinyLLM/`: training + evaluation harness with dataset/model artifact caching and timestamped HTML+JSON comparison reports.

Not yet integrated end-to-end (design intent remains, wiring is incomplete):

- Using DS005’s `PatternMiner` / `SchemaProposer` / `Consolidator` as the macro-unit discovery backend for DS011 (the current `MacroUnitModel` performs its own bounded subsequence mining and MDL promotion).
- VSA-accelerated macro-unit retrieval during continuation (current `MacroUnitModel` is n-gram/trie based).
- Making VM-conditioned continuation the default response path for query answering (the default path remains execution + bounded closure + deterministic rendering).

## Terminology

- **Token**: Minimal discrete unit from the event stream (byte, text token, visual token, etc.).
- **Macro-Unit**: A learned sequence of tokens that compresses well under MDL (DS005/DS001). Replaces "phrase" in previous drafts.
- **Separator**: Structural boundary discovered via DS010 (paragraph, scene, function).
- **Proposal**: Candidate next macro-unit generated by the language model.
- **Decoder**: Selection mechanism that chooses the next macro-unit under budgets and constraints.

## Architectural Overview

The generative path integrates with DS005's two-loop architecture:

### Outer Loop (This Document)

1. **Segmentation**: Convert event stream into macro-units using MDL compression (not just separators).
2. **Conditioning**: Encode current VM state into a compact conditioning signal.
3. **Proposal**: Generate next macro-unit candidates from the conditioned model.
4. **Validation + Decoding**: Filter proposals through VM constraints and select next macro-unit.

### Inner Loop (DS005, executed during training)

The repository contains DS005 inner-loop components (rule/schema learning via `PatternMiner`, `SchemaProposer`, `Consolidator`).
In the current codebase, DS011 macro-unit discovery is implemented inside `MacroUnitModel` rather than being driven by those DS005 components.
Future work can unify these paths so that inner-loop consolidation feeds the outer-loop macro-unit vocabulary.

Generation remains proposal + validation, not free-form continuation.

## Macro-Unit Discovery (Normative)

Macro-units are NOT the same as separators:
- **Separators** (DS010): Boundaries between structural contexts (paragraph breaks, scene cuts).
- **Macro-units** (DS011): Reusable token sequences that compress well under MDL.

### MDL-Based Macro-Unit Learning

Macro-unit discovery follows DS005's compression-driven learning:

```
FUNCTION discover_macro_units(token_sequence, existing_units):
    candidates = mine_frequent_subsequences(token_sequence)
    
    FOR candidate IN candidates:
        # Calculate MDL improvement
        description_cost = encode_macro_unit(candidate)
        usage_cost = count_occurrences(candidate, token_sequence) * log2(vocabulary_size)
        savings = usage_cost - description_cost - pointer_overhead
        
        IF savings > MDL_PROMOTION_THRESHOLD:
            # Validate on held-out data
            IF validates_on_holdout(candidate):
                existing_units.add(candidate)
    
    RETURN existing_units
```

For large corpora, `token_sequence` is processed in streaming batches with sampling and pruning,
and `mine_frequent_subsequences` operates on bounded buffers rather than the full corpus in memory.

### Macro-Unit Schema

```javascript
{
  unitId: "mu_001",
  tokens: [0x48, 0x65, 0x6c, 0x6c, 0x6f],  // "Hello"
  frequency: 1523,
  mdlScore: 0.73,
  contextSignature: "hv:...",  // VSA signature for retrieval
  promotedAt: 1737451234567,
  version: 1
}
```

## Interfaces (Normative)

### Macro-Unit Model Interface

```javascript
class MacroUnitModel {
  /**
   * Train on token sequences, discovering and consolidating macro-units.
   * In this repository, macro-unit mining and MDL promotion are performed
   * inside MacroUnitModel (bounded subsequence counts + pruning/sampling).
   */
  async train(tokenSequences, vmStateSignatures) {}

  /**
   * Train on a streaming iterator of token sequences.
   * Required for memory-bounded training on large corpora.
   */
  async trainStream(tokenSequenceStream, options) {}
  
  /**
   * Propose next macro-units given context.
   * Returns ranked candidates with scores.
   */
  async propose(contextTokens, vmStateSignature, limit) {}
  
  /**
   * Get all consolidated macro-units.
   */
  getMacroUnits() {}
  
  /**
   * Encode token sequence using learned macro-units (compression).
   */
  encode(tokens) {}
  
  /**
   * Decode macro-unit sequence back to tokens.
   */
  decode(macroUnitIds) {}
}
```

### VM State Conditioner Interface

```javascript
class VMStateConditioner {
  /**
   * Encode VM state into deterministic signature.
   * Independent of domain labels (NFS11).
   */
  encode(vmState) {}
}
```

### Decoder Interface

```javascript
class Decoder {
  /**
   * Select next macro-unit from validated proposals.
   */
  async step(state, proposals, constraints) {}
}
```

### Generation State

```javascript
{
  tokens: number[],             // Generated token sequence so far
  macroUnits: string[],         // Macro-unit IDs used
  scopePath: string[],          // Structural context path (DS010)
  vmStateSignature: string,     // Conditioner output
  budget: { maxTokens, maxTimeMs, maxSteps },
  mode: "STRICT" | "CONDITIONAL" | "INDETERMINATE"
}
```

## Training Integration (Normative)

DS011 generation depends on DS005 training. The training pipeline must:

### Phase 1: Ingest Events
```javascript
// Already exists in train-vsavm.mjs
await vm.ingestEvents(events, { sourceId });
```

### Phase 2: Discover Separators (DS010)
```javascript
// Emergent separator discovery
const separators = await separatorDetector.detectSeparators(events);
```

### Phase 3: Mine Token Patterns (DS005 Inner Loop)
```javascript
// Convert events to token sequence
const tokens = events.map(e => e.payload.byte);

// Mine frequent patterns
const patterns = patternMiner.mineAll(tokens);
```

### Memory-Bounded Training (Normative)

Large-corpus training MUST be memory bounded and streaming:

- Token sequences are processed as a stream (no full corpus in memory).
- N-gram counts are capped at a maximum order (default: 8) to prevent explosive growth.
- Macro-unit mining uses sampling + periodic pruning of low-frequency subsequences.
- Training harnesses set an explicit heap budget (leave 4–5 GB free for the OS).

Recommended streaming pattern:

```javascript
const model = new MacroUnitModel({
  maxNgramOrder: 8,
  maxSubsequenceLength: 16,
  maxSubsequenceEntries: 500000,
  subsequenceSampleRate: 0.5
});

await model.trainStream(sequenceStream, {
  onProgress: ({ sequences, totalBytes }) => logProgress(...)
});
```

### Phase 4: Propose Macro-Units (DS005 Inner Loop)
```javascript
// Propose macro-units from patterns
const candidates = schemaProposer.proposeFromPatterns(patterns, {
  minSupport: 5,
  minLength: 2,
  maxLength: 32
});
```

### Phase 5: Consolidate via MDL (DS005 Inner Loop)
```javascript
// Evaluate and promote macro-units
for (const candidate of candidates) {
  const decision = consolidator.evaluate(candidate);
  if (decision.action === 'promote') {
    consolidator.promote(candidate, decision);
  }
}
```

### Phase 6: Train Prediction Model
```javascript
// Train next-macro-unit prediction conditioned on VM state
await macroUnitModel.train(tokenSequences, vmStateSignatures);

// Memory-bounded alternative for large corpora
await macroUnitModel.trainStream(tokenSequenceStream, {
  maxSubsequenceEntries: 500000,
  maxSubsequenceLength: 16
});
```

### Phase 7: Save Learned Artifacts
```javascript
// Save facts (existing)
await saveFacts(vm, factsPath);

// Save macro-units (NEW)
await saveMacroUnits(consolidator.getAllPromoted(), macroUnitsPath);

// Save prediction model (NEW)
await macroUnitModel.save(modelPath);
```

## Conditioning Signal (Normative)

The conditioning signal must be:

- Deterministic for the same VM state (strict mode).
- Compact and independent of domain labels.
- Constructed from structural state only.

Minimal signature fields:

- Fact count within active scope
- Rule count
- Scope depth
- Recent token context hash
- Budget consumption

The signature is NOT a truth signal; it guides retrieval and prediction.

## Proposal Generation (Normative)

The macro-unit model generates ranked proposals:

```javascript
[
  { unitId: "mu_001", tokens: [0x48, 0x65], score: 0.42 },
  { unitId: "mu_002", tokens: [0x54, 0x68], score: 0.31 }
]
```

Proposal generation must:

- Use recent token context (sliding window).
- Incorporate VM state signature.
- Prefer macro-units that maximize compression (MDL).
- Be deterministic in strict mode (tie-breaking by unitId).

### Fallback to Byte-Level

When no macro-unit matches with sufficient confidence:

```javascript
IF best_proposal.score < PROPOSAL_THRESHOLD:
    // Fall back to byte-level prediction
    RETURN predict_single_byte(context, vmState)
```

## Validation and Gating (Normative)

Each candidate must be validated against VM constraints:

- If the macro-unit would assert a new factual claim, it must be supported by `claims` (DS004).
- In strict mode, candidates implying unsupported claims are rejected.
- In conditional mode, candidates may pass if explicitly marked as conditional.

Validation uses the existing `claims` object from closure results (DS004).

```javascript
FUNCTION validate_macro_unit(unit, vmState, mode):
    // Decode unit to tokens
    tokens = decode(unit.tokens)
    
    // Check if tokens form a factual assertion
    IF is_factual_assertion(tokens):
        // Must be supported by VM claims
        claim = extract_claim(tokens)
        IF NOT vmState.claims.supports(claim):
            IF mode === "STRICT":
                RETURN { valid: false, reason: "unsupported_claim" }
            ELSE:
                RETURN { valid: true, conditional: true }
    
    RETURN { valid: true }
```

## Decoding Policy

Decoding selects a candidate under budgets:

- **Strict mode**: Deterministic argmax on validated candidates.
- **Conditional mode**: Optional temperature sampling from validated candidates.
- **Indeterminate mode**: Return no generation, only trace report.

Budgets enforce:

- Max tokens (output length)
- Max time (wall-clock)
- Max steps (model forward passes)

## Generation Loop (Normative)

```
FUNCTION generate(prompt, vmState, budget, mode):
    state = initialize_state(prompt, vmState, budget, mode)
    
    WHILE NOT budget_exhausted(state):
        # Get VM state signature
        signature = conditioner.encode(vmState)
        
        # Propose next macro-units
        proposals = macroUnitModel.propose(state.tokens, signature, limit=10)
        
        # Validate against VM constraints
        valid = []
        FOR p IN proposals:
            validation = validate_macro_unit(p, vmState, mode)
            IF validation.valid:
                valid.append((p, validation))
        
        IF valid IS EMPTY:
            # No valid continuation
            IF mode === "STRICT":
                BREAK
            ELSE:
                # Try byte-level fallback
                byte = predict_single_byte(state.tokens, signature)
                state.tokens.append(byte)
                CONTINUE
        
        # Select next macro-unit
        selected = decoder.step(state, valid, mode)
        
        # Append tokens
        state.tokens.extend(selected.unit.tokens)
        state.macroUnits.append(selected.unit.unitId)
        
        # Check for natural end
        IF is_end_of_generation(selected):
            BREAK
    
    RETURN {
        tokens: state.tokens,
        macroUnits: state.macroUnits,
        budgetUsed: state.budget.used()
    }
```

## Evaluation Metrics (Normative)

Metrics for the generative path:

### Compression Metrics (Primary)
- **Compression ratio**: `len(original) / len(encoded_with_macro_units)`
- **Macro-unit coverage**: Percentage of tokens covered by macro-units
- **MDL score**: Total description length improvement

### Prediction Metrics
- **Next-byte perplexity**: Standard LM metric on byte sequences
- **Next-macro-unit accuracy**: Top-1/Top-5 accuracy for macro-unit prediction

### Quality Metrics
- **Determinism rate**: Percentage of identical outputs for identical inputs (strict mode)
- **Constraint violations**: Must be zero in strict mode
- **Reference match**: Byte accuracy against held-out continuations

### Few-Shot Metrics
- **K-shot accuracy**: Accuracy after seeing K examples
- **Learning curve AUC**: Area under accuracy-vs-examples curve

These metrics are evaluation-only; they must not hardcode domains.

## Connection to eval_tinyLLM

`eval_tinyLLM/` is the current reproducible harness for training and comparing:

- a small TensorFlow byte-level Transformer baseline
- VSAVM’s DS011 `MacroUnitModel` (macro-unit + byte continuation)

The harness is intentionally scoped to language-model metrics (perplexity/throughput/reference match/compression/repetition) and does not attempt to validate semantic correctness claims (that remains the VM + bounded closure responsibility).

### Artifacts and results (current layout)

- Prepared datasets: `eval_tinyLLM/cache/datasets/<datasetId>/...` (+ `latest.json` pointer)
- Trained models:
  - `eval_tinyLLM/cache/models/vsavm/<datasetId>/<modelId>/...`
  - `eval_tinyLLM/cache/models/tf/<datasetId>/<modelId>/...`
- Timestamped comparison reports:
  - `eval_tinyLLM/results/<timestamp>_results.html`
  - `eval_tinyLLM/results/<timestamp>_results.json`

### VSAVM training (current pipeline)

`eval_tinyLLM/tools/train-vsavm.mjs` performs:

1. (Optional) ingest the training dataset into VSAVM to materialize facts (`--skip-ingest` disables this for large runs).
2. Stream token sequences (bytes) from `train.txt`.
3. Train `MacroUnitModel.trainStream(...)` with bounded counts, pruning, and optional sampling.
4. Export a compact model snapshot and write `meta.json` (duration, params, compression/perplexity samples) under the selected `datasetId` + `modelId`.

### VSAVM generation in the harness

The harness loads a trained macro-unit model via `eval_tinyLLM/lib/vsavm-driver.mjs` and calls:

```js
await model.generate(promptBytes, { maxTokens, budgetMs, temperature });
```

By default, the harness does not pass a VM state object to generation, so VM conditioning and claim gating are not exercised. Those mechanisms exist in `MacroUnitModel` and can be wired into a future harness variant once a stable VM-conditioned benchmark is defined.

### Comparison reports

`eval_tinyLLM/tools/compare.mjs` runs both engines under identical budgets and writes a timestamped HTML+JSON report. The report is meant to be checked into `results/` (not `cache/`) so multiple runs can coexist without overwriting.

## Implementation Notes (Non-Normative)

- The initial macro-unit model can be a simple n-gram model with macro-unit vocabulary.
- VSA can accelerate macro-unit retrieval via similarity search on context signatures.
- Later implementations may use more sophisticated models while preserving interfaces.
- Any evaluation-only baselines (e.g., TensorFlow) must remain outside core modules.

## Alignment with Existing Specs

| Spec | Alignment |
|------|-----------|
| DS001 | Macro-units implement the macro-unit layer (formerly called the “phrase layer”); reversible to token layer |
| DS002 | VM executes, decoder validates; separation preserved |
| DS003 | Query compilation orthogonal to generation |
| DS004 | Claims gating via validation step; mode handling preserved |
| DS005 | Inner loop = macro-unit discovery; outer loop = generation |
| DS010 | Separators define context boundaries; macro-units are content compression |
| NFS | No new dependencies; deterministic strict mode; no hardcoded domains |
| NFS11 | No domain-specific logic in conditioning or proposal |
| FS06 | Generate continuation candidates conditioned on VM state |
| FS07 | Decode VM results into text; no new facts beyond claims |

DS011 does not alter these requirements. It operationalizes the generative path so VSAVM can function as an LLM while maintaining correctness guarantees.
