<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Reinforcement Learning (RL) | VSAVM</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Fraunces:wght@400;600&family=Space+Grotesk:wght@400;500;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../assets/site.css">
  </head>
  <body>
    <div class="site">
      <header class="header">
        <div class="brand">VSAVM</div>
        <nav class="nav">
          <a href="../index.html">Home</a>
          <a href="../specs/">Specs</a>
          <a href="../theory/index.html">Theory</a>
          <a href="../wiki/index.html">Wiki</a>
        </nav>
      </header>
      <main class="main content">
        <h1>Reinforcement Learning (RL)</h1>
<p>This wiki entry defines a term used across VSAVM and explains why it matters in the architecture.</p><p>The diagram has a transparent background and highlights the operational meaning of the term inside VSAVM.</p>
<p>Related wiki pages: <a href="vm.html">VM</a>, <a href="event-stream.html">event stream</a>, <a href="vsa.html">VSA</a>, <a href="bounded-closure.html">bounded closure</a>, <a href="consistency-contract.html">consistency contract</a>.</p>
<h2>Definition</h2>
<p>Reinforcement learning learns preferences over actions using feedback signals such as rewards and penalties.</p>
<h2>Role in VSAVM</h2>
<p>VSAVM uses RL as shaping when multiple plausible candidates exist. The goal is to select interpretations and response modes that remain stable under bounded closure, not to optimize token-by-token behavior.</p>
<h2>Mechanics and implications</h2>
<p>The action space is coarse: choose a schema, choose a macro program, choose a response mode. Closure-derived contradictions provide negative signals that discourage unstable choices. RL complements, but does not replace, explicit closure gating.</p>
<h2>Further reading</h2>
<p>RL is a broad area. VSAVMâ€™s practical use is closer to bandit-like shaping than to full on-policy token-level control.</p>
<figure class="diagram">
<svg viewBox="0 0 900 320" role="img" aria-label="RL diagram">
  <defs>
    <linearGradient id="sky" x1="0" y1="0" x2="1" y2="1">
      <stop offset="0" stop-color="#e8f3ff"/>
      <stop offset="1" stop-color="#d6f5e8"/>
    </linearGradient>
    <linearGradient id="deep" x1="0" y1="0" x2="1" y2="1">
      <stop offset="0" stop-color="#0b6eff"/>
      <stop offset="1" stop-color="#16b879"/>
    </linearGradient>
  </defs>
  <rect x="90" y="70" rx="18" ry="18" width="240" height="70" fill="url(#sky)" stroke="#7fb3e6" stroke-width="2"/>
<text x="210.0" y="109.0" text-anchor="middle" font-size="13" fill="#0b1a2b" font-family="Space Grotesk">Choose</text>
<rect x="350" y="70" rx="18" ry="18" width="240" height="70" fill="url(#sky)" stroke="#7fb3e6" stroke-width="2"/>
<text x="470.0" y="109.0" text-anchor="middle" font-size="13" fill="#0b1a2b" font-family="Space Grotesk">Feedback</text>
<rect x="610" y="70" rx="18" ry="18" width="240" height="70" fill="url(#sky)" stroke="#7fb3e6" stroke-width="2"/>
<text x="730.0" y="109.0" text-anchor="middle" font-size="13" fill="#0b1a2b" font-family="Space Grotesk">Update</text>
<line x1="330" y1="105" x2="350" y2="105" stroke="url(#deep)" stroke-width="4" stroke-linecap="round"/>
<polygon points="340,98 340,112 360,105" fill="#16b879"/>
<line x1="590" y1="105" x2="610" y2="105" stroke="url(#deep)" stroke-width="4" stroke-linecap="round"/>
<polygon points="600,98 600,112 620,105" fill="#16b879"/>
<rect x="210" y="170" rx="18" ry="18" width="520" height="70" fill="url(#sky)" stroke="#7fb3e6" stroke-width="2"/>
<text x="470.0" y="209.0" text-anchor="middle" font-size="13" fill="#0b1a2b" font-family="Space Grotesk">Penalty when closure finds contradictions</text>
<line x1="470" y1="140" x2="470" y2="170" stroke="#0b6eff" stroke-width="4" stroke-linecap="round"/>
<polygon points="460,163 460,177 480,170" fill="#16b879"/>
<rect x="90" y="255" width="360" height="78" rx="16" ry="16" fill="none" stroke="#7fb3e6" stroke-width="2"/>
<text x="106" y="277" text-anchor="start" font-size="12" fill="#2f4a63" font-family="Space Grotesk">Legend</text>
<text x="106" y="299" text-anchor="start" font-size="12" fill="#2f4a63" font-family="Space Grotesk">Used as shaping in VSAVM.</text>
<text x="106" y="317" text-anchor="start" font-size="12" fill="#2f4a63" font-family="Space Grotesk">Acts on program choices, not tokens.</text>
<text x="106" y="335" text-anchor="start" font-size="12" fill="#2f4a63" font-family="Space Grotesk">Consistency provides key signals.</text>
</svg>
<figcaption>RL supplies shaping signals that bias high-level choices toward stable candidates.</figcaption>
</figure>
<h2>References</h2>
<p><a href="https://en.wikipedia.org/wiki/Reinforcement_learning">Reinforcement learning (Wikipedia)</a> <a href="http://incompleteideas.net/book/the-book-2nd.html">Sutton & Barto (book)</a> <a href="https://en.wikipedia.org/wiki/Multi-armed_bandit">Multi-armed bandit (Wikipedia)</a></p>
      </main>
      <footer class="footer">
        VSAVM is an Axiologic Research experiment within the Achilles project. This static documentation is written in clear academic English for engineers.
      </footer>
    </div>
  </body>
</html>
